<!doctype html><html lang=en dir=" auto"><head><meta name=description content="Olin Johnson's personal programming blog"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Neural Networks Part 3: Neurons to Layers | Olin Johnson</title><meta name=keywords content><meta name=description content="Extrapolating neuron code to layers for added efficiency and scale of the network"><meta name=author content="Olin Johnson"><link rel=canonical href=https://olinjohnson.github.io/posts/neural-networks-part-3-neurons-to-layers/><link crossorigin=anonymous href=/assets/css/stylesheet.7d9f7c64bac03319df5e9e8f49e4af1e2a1e2f65ebc47459ef6a3a495f423b1a.css integrity="sha256-fZ98ZLrAMxnfXp6PSeSvHioeL2XrxHRZ72o6SV9COxo=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://olinjohnson.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://olinjohnson.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://olinjohnson.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://olinjohnson.github.io/apple-touch-icon.png><link rel=mask-icon href=https://olinjohnson.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Neural Networks Part 3: Neurons to Layers"><meta property="og:description" content="Extrapolating neuron code to layers for added efficiency and scale of the network"><meta property="og:type" content="article"><meta property="og:url" content="https://olinjohnson.github.io/posts/neural-networks-part-3-neurons-to-layers/"><meta property="og:image" content="https://olinjohnson.github.io/posts/neural_networks_series/images/layer_cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-28T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-28T00:00:00+00:00"><meta property="og:site_name" content="Olin Johnson"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://olinjohnson.github.io/posts/neural_networks_series/images/layer_cover.png"><meta name=twitter:title content="Neural Networks Part 3: Neurons to Layers"><meta name=twitter:description content="Extrapolating neuron code to layers for added efficiency and scale of the network"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://olinjohnson.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Neural Networks Part 3: Neurons to Layers","item":"https://olinjohnson.github.io/posts/neural-networks-part-3-neurons-to-layers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Neural Networks Part 3: Neurons to Layers","name":"Neural Networks Part 3: Neurons to Layers","description":"Extrapolating neuron code to layers for added efficiency and scale of the network","keywords":[],"articleBody":"If you haven’t been following along with the previous posts in this neural networks series, I highly recommend reading those before continuing forward. At this point in the series, we’ve talked about layers, and we’ve talked about coding neurons. Now, we’re going to scale up the neuron code to be applicable for layers of the network, to make it more customizable and functional.\nWhy are we doing this? The reason we can’t just take the neuron code from Part 2 and move forward is because it isn’t really practical in the context of our network. If you recall, we created a class for each neuron in the network, but with neural networks getting increasingly large (most networks have thousands to millions of neurons), it’s simply too tedious and too slow to instantiate a new instance of the class for each neuron in the network. That approach might be manageable for the simple 6-neuron example network I presented, but it won’t be realistic for larger-scale networks with practical functionality.\nThus, we need to scale our neuron code to work with layers in our network.\nWhat are we going to do instead? Well, instead of having a class for each neuron in the network, it would be more realistic to define a class that represents each layer in the network.\nRemember, networks can have any number of hidden layers, so by creating a class to represent network layers, we can make our network functional, practical, and scalable.\nAlright, how is this going to work? Well, in order to move forward, we’re going to need to change up how we’ve been thinking. We’ll need to start using matrices to describe things like weights and biases.\nFor example, consider the following model:\nNotice how each neuron has 2 weights. Remember that each weight is just a number, so we can give each neuron its own weight vector to keep track of them.\nFor example, the weight vector of the neuron \\(y_1\\) would be\n$$\\begin{bmatrix}w_2 \u0026 w_3\\end{bmatrix}$$\nBecause those are the weights that connect the neuron to its inputs.\nFortunately, since each neuron only has 1 bias, we don’t need to use a vector to represent the bias. We can reference the neuron’s bias using the notation \\(b_n\\), where \\(n\\) is the corresponding numbered neuron.\n Writing the Layer class Now that we’ve covered how to represent the neurons’ weights and biases (you’ll see why in a minute), we can start writing the code for the Layer class. We’ll start by declaring a very basic constructor method:\nclass Layer:  def __init__(self, num_inputs, num_neurons):  # ... You’ll notice that the class takes two arguments: num_inputs, for the number of input neurons, and num_neurons, for the number of neurons in the layer.\nWe need num_inputs to define the right number of weights per neuron in the layer, and we need num_neurons to know the right number of neurons that we need to define weights for.\nThe reason we need these parameters is to create a weight matrix. This is essentially a vector of all the neurons’ weights. For instance, since each neuron’s weights are represented by a vector, to represent the weights of all the neurons in the layer, we would need a vector of those vectors, or a matrix.\nFor example, the weight matrix of the model from above would be:\n$$ \\begin{bmatrix} w_0 \u0026 w_1 \\\\ w_2 \u0026 w_3 \\\\ w_4 \u0026 w_5 \\end{bmatrix} $$\nYou can see how each row in the matrix represents a neuron, and each column represents one of the neuron’s weights. Thus, the number of rows in the matrix is equal to the number of neurons, and the number of columns is equal to the number of weights per neuron, which is equal to the total number of inputs.\nMaking sense?\nSo, now we can randomly initialize all the weights for our neurons at once by creating a matrix with dimensions num_neurons * num_inputs. Check out the following code:\nclass Layer:  def __init__(self, num_inputs, num_neurons):  self.weights = np.random.randn(num_neurons, num_inputs)  # ... Note: np.random.randn() is a numpy function that returns samples from a standard normal distribution. By passing matrix dimensions as arguments, the function is able to populate an entire matrix with random values. You can view the documentation for np.random.randn() here.  We can do something similar with the biases, though we only need a vector to hold all the biases for the layer because each neuron only has one bias.\nclass Layer:  def __init__(self, num_inputs, num_neurons):  self.weights = np.random.randn(num_neurons, num_inputs)  self.biases = np.random.randn(num_neurons) Calculating neuron values (and code!) By representing weights, inputs, and biases using matrices like above, we can speed up the network significantly and calculate the values of every neuron at once, instead of individually like we were doing in part 2.\nTo do this, we can find the dot product of the inputs and weights.\n$$ X = I * W + B $$\nNote: in the future, I’ll write a more in-depth post on what a dot product is, how to compute a dot product, and their application in neural networks, but for now, I think the mathisfun guide does a pretty good job explaining. We can represent this in code by defining a method to our Layer class that returns the dot product of its inputs and weights (and adds the biases) using the numpy function np.dot().\nclass Layer:  def __init__(self, num_inputs, num_neurons):  self.weights = np.random.randn(num_neurons, num_inputs)  self.biases = np.random.randn(num_neurons)  def calc_neuron_outputs(self, inputs):  # assert len(inputs) == len(biases)  return np.dot(inputs, self.weights) + self.biases As you can see, the method Layer.calc_neuron_outputs() takes a number of inputs (one for each neuron in the layer) and returns a matrix with all the output values of the neurons by computing the dot product of the inputs and weights, and then adding the biases.\nThat’s as far as I’ll go for now, but keep an eye out for future posts where we’ll dig a little deeper into dot products, batches of inputs, and activation functions.\n .toc { cursor: default; width:100%; padding-left:var(--gap); padding-right:var(--gap); padding-top:.6em; } .topper { border-bottom: 1px solid var(--primary); padding-bottom: .6em; padding-top: .2em; } ul { margin-top: .8em; margin-bottom: .8em!important; padding-left:var(--gap); } .nnsa { box-shadow:none!important; }  Other Parts in the Neural Networks Series ⬇️  Introduction - The Pieplatter Project Part 0 - Bits and Brains? Part 1 - Looking at Layers Part 2 - Neuron Code Part 3 - Neurons to Layers   ","wordCount":"1063","inLanguage":"en","image":"https://olinjohnson.github.io/posts/neural_networks_series/images/layer_cover.png","datePublished":"2023-08-28T00:00:00Z","dateModified":"2023-08-28T00:00:00Z","author":{"@type":"Person","name":"Olin Johnson"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://olinjohnson.github.io/posts/neural-networks-part-3-neurons-to-layers/"},"publisher":{"@type":"Organization","name":"Olin Johnson","logo":{"@type":"ImageObject","url":"https://olinjohnson.github.io/favicon.ico"}}}</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-8Y5HYW1N1F"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8Y5HYW1N1F")</script><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><body id=top id="
    top"><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://olinjohnson.github.io/ accesskey=h title="Olin Johnson (Alt + H)">Olin Johnson</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://olinjohnson.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://olinjohnson.github.io/about/ title=About><span>About</span></a></li><li><a href=https://olinjohnson.github.io/projects title=Projects><span>Projects</span></a></li><li><a href=https://olinjohnson.github.io/categories title=Categories><span>Categories</span></a></li><li><a href=https://olinjohnson.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Neural Networks Part 3: Neurons to Layers</h1><div class=post-description>Extrapolating neuron code to layers for added efficiency and scale of the network</div><div class=post-meta><span title="2023-08-28 00:00:00 +0000 UTC">August 28, 2023</span>&nbsp;·&nbsp;Olin Johnson</div></header><figure class=entry-cover><img loading=lazy src=https://olinjohnson.github.io/posts/neural_networks_series/images/layer_cover.png alt="cover image"></figure><div class=post-content><div style=background-color:var(--entry);margin-bottom:20px;margin-top:10px;padding:10px;border-radius:var(--radius)><em>If you haven&rsquo;t been following along with the previous posts in this neural networks series, I highly recommend reading those before continuing forward.</em></div><p>At this point in the series, we&rsquo;ve talked about layers, and we&rsquo;ve talked about coding neurons. Now, we&rsquo;re going to scale up the neuron code to be applicable for layers of the network, to make it more customizable and functional.</p><h3 id=why-are-we-doing-this>Why are we doing this?<a hidden class=anchor aria-hidden=true href=#why-are-we-doing-this>#</a></h3><p>The reason we can&rsquo;t just take the neuron code from <a href=/posts/neural-networks-part-2-neuron-code>Part 2</a> and move forward is because it isn&rsquo;t really practical in the context of our network. If you recall, we created a class for each neuron in the network, but with neural networks getting increasingly large (most networks have thousands to millions of neurons), it&rsquo;s simply too tedious and too slow to instantiate a new instance of the class for each neuron in the network. That approach might be manageable for the simple 6-neuron example network I presented, but it won&rsquo;t be realistic for larger-scale networks with practical functionality.</p><p>Thus, we need to scale our neuron code to work with layers in our network.</p><h3 id=what-are-we-going-to-do-instead>What are we going to do instead?<a hidden class=anchor aria-hidden=true href=#what-are-we-going-to-do-instead>#</a></h3><p>Well, instead of having a class for each neuron in the network, it would be more realistic to define a class that represents each layer in the network.</p><p>Remember, networks can have any number of hidden layers, so by creating a class to represent network layers, we can make our network functional, practical, and scalable.</p><h3 id=alright-how-is-this-going-to-work>Alright, how is this going to work?<a hidden class=anchor aria-hidden=true href=#alright-how-is-this-going-to-work>#</a></h3><p>Well, in order to move forward, we&rsquo;re going to need to change up how we&rsquo;ve been thinking. We&rsquo;ll need to start using matrices to describe things like weights and biases.</p><p>For example, consider the following model:</p><p><img loading=lazy src=/posts/neural_networks_series/images/big_net_model.png alt="example image"></p><p>Notice how each neuron has 2 weights. Remember that each weight is just a number, so we can give each neuron its own weight vector to keep track of them.</p><p>For example, the weight vector of the neuron \(y_1\) would be</p><p>$$\begin{bmatrix}w_2 & w_3\end{bmatrix}$$</p><p>Because those are the weights that connect the neuron to its inputs.</p><p>Fortunately, since each neuron only has 1 bias, we don&rsquo;t need to use a vector to represent the bias. We can reference the neuron&rsquo;s bias using the notation \(b_n\), where \(n\) is the corresponding numbered neuron.</p><hr><h3 id=writing-the-layer-class>Writing the <code>Layer</code> class<a hidden class=anchor aria-hidden=true href=#writing-the-layer-class>#</a></h3><p>Now that we&rsquo;ve covered how to represent the neurons&rsquo; weights and biases (you&rsquo;ll see why in a minute), we can start writing the code for the <code>Layer</code> class. We&rsquo;ll start by declaring a very basic constructor method:</p><div class=highlight><pre tabindex=0 style=color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#81a1c1;font-weight:700>class</span> <span style=color:#8fbcbb>Layer</span><span style=color:#eceff4>:</span>
</span></span><span style=display:flex><span>    <span style=color:#81a1c1;font-weight:700>def</span> __init__<span style=color:#eceff4>(</span>self<span style=color:#eceff4>,</span> num_inputs<span style=color:#eceff4>,</span> num_neurons<span style=color:#eceff4>):</span>
</span></span><span style=display:flex><span>        <span style=color:#616e87;font-style:italic># ...</span></span></span></code></pre></div><p>You&rsquo;ll notice that the class takes two arguments: <code>num_inputs</code>, for the number of input neurons, and <code>num_neurons</code>, for the number of neurons in the layer.</p><p>We need <code>num_inputs</code> to define the right number of weights per neuron in the layer, and we need <code>num_neurons</code> to know the right number of neurons that we need to define weights for.</p><p>The reason we need these parameters is to create a <strong>weight matrix</strong>. This is essentially a vector of all the neurons&rsquo; weights. For instance, since each neuron&rsquo;s weights are represented by a vector, to represent the weights of all the neurons in the layer, we would need a vector of those vectors, or a <strong>matrix</strong>.</p><p>For example, the weight matrix of the model from above would be:</p><p>$$
\begin{bmatrix}
w_0 & w_1 \\
w_2 & w_3 \\
w_4 & w_5
\end{bmatrix}
$$</p><p>You can see how each row in the matrix represents a neuron, and each column represents one of the neuron&rsquo;s weights. Thus, the number of rows in the matrix is equal to the number of neurons, and the number of columns is equal to the number of weights per neuron, which is equal to the total number of inputs.</p><p>Making sense?</p><p>So, now we can randomly initialize all the weights for our neurons at once by creating a matrix with dimensions <code>num_neurons</code> * <code>num_inputs</code>. Check out the following code:</p><p><div class=highlight><pre tabindex=0 style=color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#81a1c1;font-weight:700>class</span> <span style=color:#8fbcbb>Layer</span><span style=color:#eceff4>:</span>
</span></span><span style=display:flex><span>    <span style=color:#81a1c1;font-weight:700>def</span> __init__<span style=color:#eceff4>(</span>self<span style=color:#eceff4>,</span> num_inputs<span style=color:#eceff4>,</span> num_neurons<span style=color:#eceff4>):</span>
</span></span><span style=display:flex><span>        self<span style=color:#81a1c1>.</span>weights <span style=color:#81a1c1>=</span> np<span style=color:#81a1c1>.</span>random<span style=color:#81a1c1>.</span>randn<span style=color:#eceff4>(</span>num_neurons<span style=color:#eceff4>,</span> num_inputs<span style=color:#eceff4>)</span>
</span></span><span style=display:flex><span>        <span style=color:#616e87;font-style:italic># ...</span></span></span></code></pre></div><div style=font-size:12pt;font-style:italic;background-color:var(--entry);margin-bottom:20px;margin-top:10px;padding:10px;border-radius:var(--radius)>Note: np.random.randn() is a numpy function that returns samples from a <a href=https://en.wikipedia.org/wiki/Normal_distribution>standard normal distribution</a>. By passing matrix dimensions as arguments, the function is able to populate an entire matrix with random values. You can view the documentation for np.random.randn() <a href=https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html>here</a>.</div></p><p>We can do something similar with the biases, though we only need a vector to hold all the biases for the layer because each neuron only has one bias.</p><div class=highlight><pre tabindex=0 style=color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#81a1c1;font-weight:700>class</span> <span style=color:#8fbcbb>Layer</span><span style=color:#eceff4>:</span>
</span></span><span style=display:flex><span>    <span style=color:#81a1c1;font-weight:700>def</span> __init__<span style=color:#eceff4>(</span>self<span style=color:#eceff4>,</span> num_inputs<span style=color:#eceff4>,</span> num_neurons<span style=color:#eceff4>):</span>
</span></span><span style=display:flex><span>        self<span style=color:#81a1c1>.</span>weights <span style=color:#81a1c1>=</span> np<span style=color:#81a1c1>.</span>random<span style=color:#81a1c1>.</span>randn<span style=color:#eceff4>(</span>num_neurons<span style=color:#eceff4>,</span> num_inputs<span style=color:#eceff4>)</span>
</span></span><span style=display:flex><span>        self<span style=color:#81a1c1>.</span>biases <span style=color:#81a1c1>=</span> np<span style=color:#81a1c1>.</span>random<span style=color:#81a1c1>.</span>randn<span style=color:#eceff4>(</span>num_neurons<span style=color:#eceff4>)</span></span></span></code></pre></div><h3 id=calculating-neuron-values-and-code>Calculating neuron values (and code!)<a hidden class=anchor aria-hidden=true href=#calculating-neuron-values-and-code>#</a></h3><p>By representing weights, inputs, and biases using matrices like above, we can speed up the network significantly and calculate the values of every neuron at once, instead of individually like we were doing in <a href=/posts/neural-networks-part-2-neuron-code>part 2</a>.</p><p>To do this, we can find the <a href=https://www.mathsisfun.com/algebra/vectors-dot-product.html>dot product</a> of the inputs and weights.</p><p>$$ X = I * W + B $$</p><div style=background-color:var(--entry);margin-bottom:20px;margin-top:10px;padding:10px;border-radius:var(--radius)>Note: in the future, I&rsquo;ll write a more in-depth post on what a dot product is, how to compute a dot product, and their application in neural networks, but for now, I think the <a href=https://www.mathsisfun.com/algebra/vectors-dot-product.html>mathisfun guide</a> does a pretty good job explaining.</div><p>We can represent this in code by defining a method to our <code>Layer</code> class that returns the dot product of its inputs and weights (and adds the biases) using the numpy function <code>np.dot()</code>.</p><div class=highlight><pre tabindex=0 style=color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#81a1c1;font-weight:700>class</span> <span style=color:#8fbcbb>Layer</span><span style=color:#eceff4>:</span>
</span></span><span style=display:flex><span>    <span style=color:#81a1c1;font-weight:700>def</span> __init__<span style=color:#eceff4>(</span>self<span style=color:#eceff4>,</span> num_inputs<span style=color:#eceff4>,</span> num_neurons<span style=color:#eceff4>):</span>
</span></span><span style=display:flex><span>        self<span style=color:#81a1c1>.</span>weights <span style=color:#81a1c1>=</span> np<span style=color:#81a1c1>.</span>random<span style=color:#81a1c1>.</span>randn<span style=color:#eceff4>(</span>num_neurons<span style=color:#eceff4>,</span> num_inputs<span style=color:#eceff4>)</span>
</span></span><span style=display:flex><span>        self<span style=color:#81a1c1>.</span>biases <span style=color:#81a1c1>=</span> np<span style=color:#81a1c1>.</span>random<span style=color:#81a1c1>.</span>randn<span style=color:#eceff4>(</span>num_neurons<span style=color:#eceff4>)</span>
</span></span><span style=display:flex><span>    <span style=color:#81a1c1;font-weight:700>def</span> <span style=color:#88c0d0>calc_neuron_outputs</span><span style=color:#eceff4>(</span>self<span style=color:#eceff4>,</span> inputs<span style=color:#eceff4>):</span>
</span></span><span style=display:flex><span>        <span style=color:#616e87;font-style:italic># assert len(inputs) == len(biases)</span>
</span></span><span style=display:flex><span>        <span style=color:#81a1c1;font-weight:700>return</span> np<span style=color:#81a1c1>.</span>dot<span style=color:#eceff4>(</span>inputs<span style=color:#eceff4>,</span> self<span style=color:#81a1c1>.</span>weights<span style=color:#eceff4>)</span> <span style=color:#81a1c1>+</span> self<span style=color:#81a1c1>.</span>biases</span></span></code></pre></div><p>As you can see, the method <code>Layer.calc_neuron_outputs()</code> takes a number of inputs (one for each neuron in the layer) and returns a matrix with all the output values of the neurons by computing the dot product of the inputs and weights, and then adding the biases.</p><p>That&rsquo;s as far as I&rsquo;ll go for now, but keep an eye out for future posts where we&rsquo;ll dig a little deeper into dot products, batches of inputs, and activation functions.</p><style>.toc{cursor:default;width:100%;padding-left:var(--gap);padding-right:var(--gap);padding-top:.6em}.topper{border-bottom:1px solid var(--primary);padding-bottom:.6em;padding-top:.2em}ul{margin-top:.8em;margin-bottom:.8em!important;padding-left:var(--gap)}.nnsa{box-shadow:none!important}</style><div class=toc><div class=topper>Other Parts in the Neural Networks Series &nbsp; ⬇️</div><ul><li><a class=nnsa href=/posts/the-pieplatter-project>Introduction - The Pieplatter Project</a></li><li><a class=nnsa href=/posts/neural-networks-part-0-bits-and-brains>Part 0 - Bits and Brains?</a></li><li><a class=nnsa href=/posts/neural-networks-part-1-looking-at-layers>Part 1 - Looking at Layers</a></li><li><a class=nnsa href=/posts/neural-networks-part-2-neuron-code>Part 2 - Neuron Code</a></li><li><a class=nnsa href=/posts/neural-networks-part-3-neurons-to-layers>Part 3 - Neurons to Layers</a></li></ul></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://olinjohnson.github.io/posts/neural-networks-part-2-neuron-code/><span class=title>« Prev</span><br><span>Neural Networks Part 2: Neuron Code</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Neural Networks Part 3: Neurons to Layers on twitter" href="https://twitter.com/intent/tweet/?text=Neural%20Networks%20Part%203%3a%20Neurons%20to%20Layers&url=https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-3-neurons-to-layers%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Networks Part 3: Neurons to Layers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-3-neurons-to-layers%2f&title=Neural%20Networks%20Part%203%3a%20Neurons%20to%20Layers&summary=Neural%20Networks%20Part%203%3a%20Neurons%20to%20Layers&source=https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-3-neurons-to-layers%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Networks Part 3: Neurons to Layers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-3-neurons-to-layers%2f&title=Neural%20Networks%20Part%203%3a%20Neurons%20to%20Layers"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Networks Part 3: Neurons to Layers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-3-neurons-to-layers%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Networks Part 3: Neurons to Layers on whatsapp" href="https://api.whatsapp.com/send?text=Neural%20Networks%20Part%203%3a%20Neurons%20to%20Layers%20-%20https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-3-neurons-to-layers%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Networks Part 3: Neurons to Layers on telegram" href="https://telegram.me/share/url?text=Neural%20Networks%20Part%203%3a%20Neurons%20to%20Layers&url=https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-3-neurons-to-layers%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://olinjohnson.github.io/>Olin Johnson</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>