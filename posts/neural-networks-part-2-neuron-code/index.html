<!doctype html><html lang=en dir=" auto"><head><meta name=description content="Olin Johnson's personal programming blog"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Neural Networks Part 2: Neuron Code | Olin Johnson</title><meta name=keywords content><meta name=description content="A more thorough examination of neurons and how to represent them in code"><meta name=author content="Olin Johnson"><link rel=canonical href=https://olinjohnson.github.io/posts/neural-networks-part-2-neuron-code/><link crossorigin=anonymous href=/assets/css/stylesheet.7d9f7c64bac03319df5e9e8f49e4af1e2a1e2f65ebc47459ef6a3a495f423b1a.css integrity="sha256-fZ98ZLrAMxnfXp6PSeSvHioeL2XrxHRZ72o6SV9COxo=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://olinjohnson.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://olinjohnson.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://olinjohnson.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://olinjohnson.github.io/apple-touch-icon.png><link rel=mask-icon href=https://olinjohnson.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Neural Networks Part 2: Neuron Code"><meta property="og:description" content="A more thorough examination of neurons and how to represent them in code"><meta property="og:type" content="article"><meta property="og:url" content="https://olinjohnson.github.io/posts/neural-networks-part-2-neuron-code/"><meta property="og:image" content="https://olinjohnson.github.io/posts/neural_networks_series/images/neuron_code_cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-20T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-20T00:00:00+00:00"><meta property="og:site_name" content="Olin Johnson"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://olinjohnson.github.io/posts/neural_networks_series/images/neuron_code_cover.png"><meta name=twitter:title content="Neural Networks Part 2: Neuron Code"><meta name=twitter:description content="A more thorough examination of neurons and how to represent them in code"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://olinjohnson.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Neural Networks Part 2: Neuron Code","item":"https://olinjohnson.github.io/posts/neural-networks-part-2-neuron-code/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Neural Networks Part 2: Neuron Code","name":"Neural Networks Part 2: Neuron Code","description":"A more thorough examination of neurons and how to represent them in code","keywords":[],"articleBody":"If you haven’t been following along with the previous posts in this neural networks series, I highly recommend reading those before continuing forward. At this point in the series, I’ve laid out the general concept for how neural networks are composed and how they can utilize interconnected layers of neurons to make predictions. In this post, we’ll check out how to compute the exact values of neurons and begin to lay down some code for a simple neural net.\nA few notes about neurons Before we dive into computing the values of neurons, there are a couple other important components that I’ve left out until now: weights and biases.\nFirst, we’ll start with weights. A neuron’s weight is a number that represents the strength of the neuron’s connection to its input.\nHere’s a model: As you can see, there’s one neuron (the red circle) connected to an input neuron. The line between them (\\(w_0\\)) represents the neuron’s weight.\nLike I said before, the weight is a relatively low number that represents the strength of the connection between a neuron and it’s inputs. More generally, a neuron’s weight is a measure of how much its input influences its output.\n Secondly, we’ll talk about the bias.\nThough I’ve chosen to represent the bias with a filled green circle (\\(b_0\\)) in the example image above, the bias is not a neuron, nor is the connection between the bias and the neuron a weight. Instead, a neuron’s bias is just a constant value that is used to offset the neuron’s output during computation. It’s similar to the variable \\(b\\) for the linear function \\(y=mx+b\\), and it’s used to adjust the barriers of the neuron’s output. The purpose of the bias will become more clear after we get into computing and programming neurons.\n Calculating the value of a neuron As you can see in the graphic, there is a fairly simple formula to compute the value of a neuron:\n$$ y = input * weight + bias $$\nThe input is represented in the graphic by the value \\(x_0\\), the weight is represented by the value \\(w_0\\), and the bias is represented by the value \\(b_0\\).\nTo find the neuron’s value, we just multiply the neuron’s input and weight together (\\(x_0 * w_0\\)) and then add the bias (\\(x_0 * w_0 + b_0\\)).\nBreaking it down, we can easily recognize the functions of the weights and biases that I talked about earlier.\nRemember when I said:\n a neuron’s weight is a measure of how much its input influences its output\n    That’s why we multiply the weight by the input - because it effectively has the power to scale our input up and down.\nAnd remember this?  a neuron’s bias is just a constant value that is used to offset the neuron’s output during computation\n    That’s why we add the \\(b\\) value at the end - because it shifts the entire value of the neuron up or down.\nBut what if a neuron has multiple inputs? Think back to the following model from Part 1:\nAs you can see, all of the neurons in the hidden layer have two inputs, not just one. You’ll also notice that they each have two weights as well, that correspond to the two inputs.\nSo, to calculate the value of a neuron with multiple inputs, we multiply each of its inputs by their respective weights, get the sum, and then add the bias.\nThe formula looks like this, where \\(n\\) is the total number of inputs to a neuron:\n$$ y=b+\\sum^{n}_{i=0}x_i * w_i $$\nFor example, take the following neuron with 2 inputs and 2 weights:\nNote: the bias isn’t always represented in the model - it’s presence is assumed Like I said before, in order to calculate a neuron’s value with multiple inputs, we multiply each input by its respective weight, sum them up, and then add the bias.\nSo to calculate the value of the green neuron, we need to do:\n$$ x_0 * w_0 + x_1 * w_1 + b $$\n Coding a neuron Alright - now that we’ve finally gotten through most of the basics for understanding how a neural network functions, we can start representing some of it with code. We’ll start very lightly by understanding how to represent a neuron and its components in code.\nI’ll do most of the coding in this series in python, but if you’re familiar with other programming languages, all the same concepts will still apply. To represent a neuron in code, we could start by creating a class that holds the neuron’s components - mainly the neuron’s weights and its bias.\nclass Neuron:  def __init__(self, num_inputs):  # initialize self.weights  # initialize self.bias You can see in the constructor method that the class will take the number of inputs (num_inputs) as an argument - this is because the number of weights that we initialize depends on the number of inputs (one weight for every input).\nFor now, we’ll start by initializing the weights and bias randomly.\nimport numpy as np class Neuron:  def __init__(self, num_inputs):  self.weights = np.random.randn(num_inputs)  self.bias = np.random.randn() We can also add a method to calculate the neuron’s value by multiplying its weights and inputs together. We’ll use a dot product for this. If you’re unsure about how dot products work, don’t worry - I’ll explain them more thoroughly in the next post.\nimport numpy as np class Neuron:  def __init__(self, num_inputs):  self.weights = np.random.randn(num_inputs)  self.bias = np.random.randn()   def calculate(self, inputs):  return np.dot(inputs, self.weights.T) + self.bias You’ll see the limitations of some of this code as we get more in-depth in the following chapters of the series, but for now, this is a good start for representing a neuron.\nKeep an eye out for the next post where we’ll take a deeper dive into neuron code and representing neurons in the context of layers.\n .toc { cursor: default; width:100%; padding-left:var(--gap); padding-right:var(--gap); padding-top:.6em; } .topper { border-bottom: 1px solid var(--primary); padding-bottom: .6em; padding-top: .2em; } ul { margin-top: .8em; margin-bottom: .8em!important; padding-left:var(--gap); } .nnsa { box-shadow:none!important; }  Other Parts in the Neural Networks Series ⬇️  Introduction - The Pieplatter Project Part 0 - Bits and Brains? Part 1 - Looking at Layers Part 2 - Neuron Code   ","wordCount":"1041","inLanguage":"en","image":"https://olinjohnson.github.io/posts/neural_networks_series/images/neuron_code_cover.png","datePublished":"2023-07-20T00:00:00Z","dateModified":"2023-07-20T00:00:00Z","author":{"@type":"Person","name":"Olin Johnson"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://olinjohnson.github.io/posts/neural-networks-part-2-neuron-code/"},"publisher":{"@type":"Organization","name":"Olin Johnson","logo":{"@type":"ImageObject","url":"https://olinjohnson.github.io/favicon.ico"}}}</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-8Y5HYW1N1F"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8Y5HYW1N1F")</script><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><body id=top id="
    top"><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://olinjohnson.github.io/ accesskey=h title="Olin Johnson (Alt + H)">Olin Johnson</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://olinjohnson.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://olinjohnson.github.io/about/ title=About><span>About</span></a></li><li><a href=https://olinjohnson.github.io/projects title=Projects><span>Projects</span></a></li><li><a href=https://olinjohnson.github.io/categories title=Categories><span>Categories</span></a></li><li><a href=https://olinjohnson.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Neural Networks Part 2: Neuron Code</h1><div class=post-description>A more thorough examination of neurons and how to represent them in code</div><div class=post-meta><span title="2023-07-20 00:00:00 +0000 UTC">July 20, 2023</span>&nbsp;·&nbsp;Olin Johnson</div></header><figure class=entry-cover><img loading=lazy src=https://olinjohnson.github.io/posts/neural_networks_series/images/neuron_code_cover.png alt="cover image"></figure><div class=post-content><div style=background-color:var(--entry);margin-bottom:20px;margin-top:10px;padding:10px;border-radius:var(--radius)><em>If you haven&rsquo;t been following along with the previous posts in this neural networks series, I highly recommend reading those before continuing forward.</em></div><p>At this point in the series, I&rsquo;ve laid out the general concept for how neural networks are composed and how they can utilize interconnected layers of neurons to make predictions. In this post, we&rsquo;ll check out how to compute the exact values of neurons and begin to lay down some code for a simple neural net.</p><h3 id=a-few-notes-about-neurons>A few notes about neurons<a hidden class=anchor aria-hidden=true href=#a-few-notes-about-neurons>#</a></h3><p>Before we dive into computing the values of neurons, there are a couple other important components that I&rsquo;ve left out until now: weights and biases.</p><p>First, we&rsquo;ll start with weights. A neuron&rsquo;s weight is a number that represents the strength of the neuron&rsquo;s connection to its input.</p><p>Here&rsquo;s a model:
<img loading=lazy src=/posts/neural_networks_series/images/weights.png alt="example image"></p><p>As you can see, there&rsquo;s one neuron (the red circle) connected to an input neuron. The line between them (\(w_0\)) represents the neuron&rsquo;s weight.</p><p>Like I said before, the weight is a relatively low number that represents the strength of the connection between a neuron and it&rsquo;s inputs. More generally, a neuron&rsquo;s weight is a measure of how much its input influences its output.</p><hr><p>Secondly, we&rsquo;ll talk about the bias.</p><p><img loading=lazy src=/posts/neural_networks_series/images/bias.png alt="example image"></p><p>Though I&rsquo;ve chosen to represent the bias with a filled green circle (\(b_0\)) in the example image above, the bias is not a neuron, nor is the connection between the bias and the neuron a weight. Instead, a neuron&rsquo;s bias is just a constant value that is used to offset the neuron&rsquo;s output during computation. It&rsquo;s similar to the variable \(b\) for the linear function \(y=mx+b\), and it&rsquo;s used to adjust the barriers of the neuron&rsquo;s output. The purpose of the bias will become more clear after we get into computing and programming neurons.</p><hr><h2 id=calculating-the-value-of-a-neuron>Calculating the value of a neuron<a hidden class=anchor aria-hidden=true href=#calculating-the-value-of-a-neuron>#</a></h2><p><img loading=lazy src=/posts/neural_networks_series/images/ncompute.png alt="example image"></p><p>As you can see in the graphic, there is a fairly simple formula to compute the value of a neuron:</p><p>$$ y = input * weight + bias $$</p><p>The input is represented in the graphic by the value \(x_0\), the weight is represented by the value \(w_0\), and the bias is represented by the value \(b_0\).</p><p>To find the neuron&rsquo;s value, we just multiply the neuron&rsquo;s input and weight together (\(x_0 * w_0\)) and then add the bias (\(x_0 * w_0 + b_0\)).</p><p>Breaking it down, we can easily recognize the functions of the weights and biases that I talked about earlier.</p><p>Remember when I said:</p><blockquote><p>a neuron&rsquo;s weight is a measure of how much its input influences its output</p><footer><strong></strong></footer></blockquote><p>That&rsquo;s why we multiply the weight by the input - because it effectively has the power to scale our input up and down.</p><p>And remember this?<blockquote><p>a neuron&rsquo;s bias is just a constant value that is used to offset the neuron&rsquo;s output during computation</p><footer><strong></strong></footer></blockquote></p><p>That&rsquo;s why we add the \(b\) value at the end - because it shifts the entire value of the neuron up or down.</p><h3 id=but-what-if-a-neuron-has-multiple-inputs>But what if a neuron has multiple inputs?<a hidden class=anchor aria-hidden=true href=#but-what-if-a-neuron-has-multiple-inputs>#</a></h3><p>Think back to the following model from <a href=/posts/neural-networks-part-1-looking-at-layers>Part 1</a>:</p><p><img loading=lazy src=/posts/neural_networks_series/images/model1.png alt="example image"></p><p>As you can see, all of the neurons in the hidden layer have two inputs, not just one. You&rsquo;ll also notice that they each have two weights as well, that correspond to the two inputs.</p><p>So, to calculate the value of a neuron with multiple inputs, we multiply each of its inputs by their respective weights, get the sum, and then add the bias.</p><p>The formula looks like this, where \(n\) is the total number of inputs to a neuron:</p><p>$$ y=b+\sum^{n}_{i=0}x_i * w_i $$</p><p>For example, take the following neuron with 2 inputs and 2 weights:</p><p><img loading=lazy src=/posts/neural_networks_series/images/multi-inputs.png alt="example image"></p><div style=background-color:var(--entry);margin-bottom:20px;margin-top:10px;padding:10px;border-radius:var(--radius)><em>Note: the bias isn&rsquo;t always represented in the model - it&rsquo;s presence is assumed</em></div><p>Like I said before, in order to calculate a neuron&rsquo;s value with multiple inputs, we multiply each input by its respective weight, sum them up, and then add the bias.</p><p>So to calculate the value of the green neuron, we need to do:</p><p>$$ x_0 * w_0 + x_1 * w_1 + b $$</p><hr><h1 id=coding-a-neuron>Coding a neuron<a hidden class=anchor aria-hidden=true href=#coding-a-neuron>#</a></h1><p>Alright - now that we&rsquo;ve finally gotten through most of the basics for understanding how a neural network functions, we can start representing some of it with code. We&rsquo;ll start very lightly by understanding how to represent a neuron and its components in code.</p><div style=background-color:var(--entry);margin-bottom:20px;margin-top:10px;padding:10px;border-radius:var(--radius)><em>I&rsquo;ll do most of the coding in this series in python, but if you&rsquo;re familiar with other programming languages, all the same concepts will still apply.</em></div><p>To represent a neuron in code, we could start by creating a class that holds the neuron&rsquo;s components - mainly the neuron&rsquo;s weights and its bias.</p><div class=highlight><pre tabindex=0 style=color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#81a1c1;font-weight:700>class</span> <span style=color:#8fbcbb>Neuron</span><span style=color:#eceff4>:</span>
</span></span><span style=display:flex><span>    <span style=color:#81a1c1;font-weight:700>def</span> __init__<span style=color:#eceff4>(</span>self<span style=color:#eceff4>,</span> num_inputs<span style=color:#eceff4>):</span>
</span></span><span style=display:flex><span>        <span style=color:#616e87;font-style:italic># initialize self.weights</span>
</span></span><span style=display:flex><span>        <span style=color:#616e87;font-style:italic># initialize self.bias</span></span></span></code></pre></div><p>You can see in the constructor method that the class will take the number of inputs (<code>num_inputs</code>) as an argument - this is because the number of weights that we initialize depends on the number of inputs (one weight for every input).</p><p>For now, we&rsquo;ll start by initializing the weights and bias randomly.</p><div class=highlight><pre tabindex=0 style=color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#81a1c1;font-weight:700>import</span> <span style=color:#8fbcbb>numpy</span> <span style=color:#81a1c1;font-weight:700>as</span> <span style=color:#8fbcbb>np</span>
</span></span><span style=display:flex><span><span style=color:#81a1c1;font-weight:700>class</span> <span style=color:#8fbcbb>Neuron</span><span style=color:#eceff4>:</span>
</span></span><span style=display:flex><span>    <span style=color:#81a1c1;font-weight:700>def</span> __init__<span style=color:#eceff4>(</span>self<span style=color:#eceff4>,</span> num_inputs<span style=color:#eceff4>):</span>
</span></span><span style=display:flex><span>        self<span style=color:#81a1c1>.</span>weights <span style=color:#81a1c1>=</span> np<span style=color:#81a1c1>.</span>random<span style=color:#81a1c1>.</span>randn<span style=color:#eceff4>(</span>num_inputs<span style=color:#eceff4>)</span>
</span></span><span style=display:flex><span>        self<span style=color:#81a1c1>.</span>bias <span style=color:#81a1c1>=</span> np<span style=color:#81a1c1>.</span>random<span style=color:#81a1c1>.</span>randn<span style=color:#eceff4>()</span></span></span></code></pre></div><p>We can also add a method to calculate the neuron&rsquo;s value by multiplying its weights and inputs together. We&rsquo;ll use a <a href=https://en.wikipedia.org/wiki/Dot_product>dot product</a> for this. <em>If you&rsquo;re unsure about how dot products work, don&rsquo;t worry - I&rsquo;ll explain them more thoroughly in the next post</em>.</p><div class=highlight><pre tabindex=0 style=color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#81a1c1;font-weight:700>import</span> <span style=color:#8fbcbb>numpy</span> <span style=color:#81a1c1;font-weight:700>as</span> <span style=color:#8fbcbb>np</span>
</span></span><span style=display:flex><span><span style=color:#81a1c1;font-weight:700>class</span> <span style=color:#8fbcbb>Neuron</span><span style=color:#eceff4>:</span>
</span></span><span style=display:flex><span>    <span style=color:#81a1c1;font-weight:700>def</span> __init__<span style=color:#eceff4>(</span>self<span style=color:#eceff4>,</span> num_inputs<span style=color:#eceff4>):</span>
</span></span><span style=display:flex><span>        self<span style=color:#81a1c1>.</span>weights <span style=color:#81a1c1>=</span> np<span style=color:#81a1c1>.</span>random<span style=color:#81a1c1>.</span>randn<span style=color:#eceff4>(</span>num_inputs<span style=color:#eceff4>)</span>
</span></span><span style=display:flex><span>        self<span style=color:#81a1c1>.</span>bias <span style=color:#81a1c1>=</span> np<span style=color:#81a1c1>.</span>random<span style=color:#81a1c1>.</span>randn<span style=color:#eceff4>()</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#81a1c1;font-weight:700>def</span> <span style=color:#88c0d0>calculate</span><span style=color:#eceff4>(</span>self<span style=color:#eceff4>,</span> inputs<span style=color:#eceff4>):</span>
</span></span><span style=display:flex><span>        <span style=color:#81a1c1;font-weight:700>return</span> np<span style=color:#81a1c1>.</span>dot<span style=color:#eceff4>(</span>inputs<span style=color:#eceff4>,</span> self<span style=color:#81a1c1>.</span>weights<span style=color:#81a1c1>.</span>T<span style=color:#eceff4>)</span> <span style=color:#81a1c1>+</span> self<span style=color:#81a1c1>.</span>bias</span></span></code></pre></div><p>You&rsquo;ll see the limitations of some of this code as we get more in-depth in the following chapters of the series, but for now, this is a good start for representing a neuron.</p><p>Keep an eye out for the next post where we&rsquo;ll take a deeper dive into neuron code and representing neurons in the context of layers.</p><style>.toc{cursor:default;width:100%;padding-left:var(--gap);padding-right:var(--gap);padding-top:.6em}.topper{border-bottom:1px solid var(--primary);padding-bottom:.6em;padding-top:.2em}ul{margin-top:.8em;margin-bottom:.8em!important;padding-left:var(--gap)}.nnsa{box-shadow:none!important}</style><div class=toc><div class=topper>Other Parts in the Neural Networks Series &nbsp; ⬇️</div><ul><li><a class=nnsa href=/posts/the-pieplatter-project>Introduction - The Pieplatter Project</a></li><li><a class=nnsa href=/posts/neural-networks-part-0-bits-and-brains>Part 0 - Bits and Brains?</a></li><li><a class=nnsa href=/posts/neural-networks-part-1-looking-at-layers>Part 1 - Looking at Layers</a></li><li><a class=nnsa href=/posts/neural-networks-part-2-neuron-code>Part 2 - Neuron Code</a></li></ul></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://olinjohnson.github.io/posts/neural-networks-part-1-looking-at-layers/><span class=title>« Prev</span><br><span>Neural Networks Part 1: Looking at Layers</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Neural Networks Part 2: Neuron Code on twitter" href="https://twitter.com/intent/tweet/?text=Neural%20Networks%20Part%202%3a%20Neuron%20Code&url=https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-2-neuron-code%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Networks Part 2: Neuron Code on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-2-neuron-code%2f&title=Neural%20Networks%20Part%202%3a%20Neuron%20Code&summary=Neural%20Networks%20Part%202%3a%20Neuron%20Code&source=https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-2-neuron-code%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Networks Part 2: Neuron Code on reddit" href="https://reddit.com/submit?url=https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-2-neuron-code%2f&title=Neural%20Networks%20Part%202%3a%20Neuron%20Code"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Networks Part 2: Neuron Code on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-2-neuron-code%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Networks Part 2: Neuron Code on whatsapp" href="https://api.whatsapp.com/send?text=Neural%20Networks%20Part%202%3a%20Neuron%20Code%20-%20https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-2-neuron-code%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Networks Part 2: Neuron Code on telegram" href="https://telegram.me/share/url?text=Neural%20Networks%20Part%202%3a%20Neuron%20Code&url=https%3a%2f%2folinjohnson.github.io%2fposts%2fneural-networks-part-2-neuron-code%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://olinjohnson.github.io/>Olin Johnson</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>